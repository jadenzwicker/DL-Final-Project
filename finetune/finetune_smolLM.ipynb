{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a950b92",
   "metadata": {},
   "source": [
    "# Notebook to Fine-Tune SmolLM2 for IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e47840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True' # more GPU memory\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#warning : need a GPU to run this code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4090f",
   "metadata": {},
   "source": [
    "## Prepare IMDb sentiment analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    # Load dataset\n",
    "    # Shuffle dataset to ensure diversity\n",
    "    dataset = load_dataset('imdb')\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    \n",
    "    # Reduce dataset size if needed\n",
    "    train_size = int(1 * len(dataset['train']))\n",
    "    test_size = int(0.2 * len(dataset['test']))\n",
    "    dataset['train'] = dataset['train'].select(range(train_size))\n",
    "    dataset['test'] = dataset['test'].select(range(test_size))\n",
    "    dataset['unsupervised'] = dataset['unsupervised'].select(range(1)) # should not be used\n",
    "    \n",
    "    # Check label distribution before tokenization\n",
    "    train_labels = [example['label'] for example in dataset['train']]\n",
    "    test_labels = [example['label'] for example in dataset['test']]\n",
    "    print(f\"Label distribution in training set: {dict((x, train_labels.count(x)) for x in set(train_labels))}\")\n",
    "    print(f\"Label distribution in testing set: {dict((x, test_labels.count(x)) for x in set(test_labels))}\")\n",
    "    \n",
    "   \n",
    "    def modify_text(t):\n",
    "        \n",
    "        t = t[:2200] # approx. ensures that this fits in 768 tokens max\n",
    "        return f'\"I loved this ! Great actors\" This movie review is positive. \"The ending was a bit disappointing. Also hard to understand.\" This movie review is negative. \"{t}\" This movie review is'\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "\n",
    "        return tokenizer(\n",
    "            [modify_text(t) for t in examples['text']],\n",
    "            truncation=True, \n",
    "            max_length=768\n",
    "        )\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "    \n",
    "    # Rename label column to match Trainer's expectation\n",
    "    tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "\n",
    "    #print sample example from train and label\n",
    "    print(tokenized_datasets['train'][4]['labels'])\n",
    "    print(tokenized_datasets['train'][5]['labels'])\n",
    "    print(tokenized_datasets['train'][6]['labels'])\n",
    "    print(tokenized_datasets['train'][7]['labels'])\n",
    "    print(tokenized_datasets['train'][8]['labels'])\n",
    "\n",
    "    return tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad7efb5",
   "metadata": {},
   "source": [
    "## Setup the custom classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Sentiment Classification Model\n",
    "class SentimentClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        # Freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Get the dimension of the base model's last hidden state\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        \n",
    "        # Remove existing classification head\n",
    "        if hasattr(base_model, 'score'):\n",
    "            delattr(base_model, 'score')\n",
    "        \n",
    "        # Add new classification layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 384),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(384, 2)  # 2 neurons for binary sentiment\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get base model outputs\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "    \n",
    "        hidden_state = outputs.hidden_states[-1][:, -2, :]\n",
    "        #Last sentence is \"This movie review is\" so -1 is \" is\" and -2 is \" review\"\n",
    "        \n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(hidden_state)\n",
    "        \n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits\n",
    "        }\n",
    "    def save_pretrained(self, save_path):\n",
    "        # Only save the trainable classifier weights and configuration\n",
    "        torch.save({\n",
    "            'classifier_state_dict': self.classifier.state_dict(),\n",
    "            'hidden_size': self.base_model.config.hidden_size\n",
    "        }, save_path)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, base_model, load_path):\n",
    "        # Recreate the model with the base model\n",
    "        model = cls(base_model)\n",
    "        \n",
    "        # Load the saved state\n",
    "        checkpoint = torch.load(load_path)\n",
    "        \n",
    "        # Ensure the hidden size matches\n",
    "        assert checkpoint['hidden_size'] == base_model.config.hidden_size, \\\n",
    "            \"Loaded model's hidden size does not match the base model\"\n",
    "        \n",
    "        # Load only the classifier weights\n",
    "        model.classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
    "        \n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223804c8",
   "metadata": {},
   "source": [
    "## Custom Trainer Callback to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a6bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SaveModelCallback(TrainerCallback):\n",
    "    def __init__(self, model, save_dir=\"custom_checkpoints\"):\n",
    "        self.model = model\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Define the save path for the current epoch\n",
    "        save_path = os.path.join(self.save_dir, f\"epoch-{state.epoch:.0f}\")\n",
    "        print(f\"Saving model at {save_path}\")\n",
    "        \n",
    "        # Save the model\n",
    "        self.model.save_pretrained(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df371c",
   "metadata": {},
   "source": [
    "## Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Simple accuracy calculation\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d798a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# Wrap base model in custom sentiment classification model\n",
    "model = SentimentClassificationModel(base_model).to(device)\n",
    "\n",
    "# Prepare datasets\n",
    "tokenized_datasets = prepare_dataset()\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10, \n",
    "    per_device_train_batch_size=90, #adjust based on GPU memory and max_length\n",
    "    per_device_eval_batch_size=90, #128 seems good for 32 VRAM and max size 768\n",
    "    warmup_steps=120,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac72b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the save directory\n",
    "save_dir = \"./custom_checkpoints_run384\"\n",
    "\n",
    "# Create the callback\n",
    "save_callback = SaveModelCallback(model, save_dir=save_dir)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[save_callback]\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "DO_TRAIN = True\n",
    "if DO_TRAIN:\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning complete!\")\n",
    "    #save\n",
    "    model.save_pretrained(\"sentiment_model_run384\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928c217",
   "metadata": {},
   "source": [
    "## Launch eval / load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661241f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from /home/bpopper/gtCode/DL-Final-Project/code/finetune/sentiment_model#\n",
    "print('Loading model from ckpt')\n",
    "model = SentimentClassificationModel.from_pretrained(base_model, \"./custom_checkpoints/epoch-1\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Inference function\n",
    "def predict_sentiment(text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.softmax(outputs['logits'], dim=1)\n",
    "        sentiment = torch.argmax(predictions, dim=1)\n",
    "        print(predictions)\n",
    "    \n",
    "    return \"Positive\" if sentiment.item() == 1 else \"Negative\"\n",
    "\n",
    "# Example usage\n",
    "example_text = \"This movie was absolutely fantastic and I loved every minute of it!\"\n",
    "print(f\"Sentiment: {predict_sentiment(example_text)}\")\n",
    "\n",
    "\n",
    "\n",
    "example_text2 = \"The movie was so bad !!\"\n",
    "print(f\"Sentiment: {predict_sentiment(example_text2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-vlm]",
   "language": "python",
   "name": "conda-env-.conda-vlm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
