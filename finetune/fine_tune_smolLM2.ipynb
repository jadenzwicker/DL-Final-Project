{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a950b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  1 20:07:36 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla V100-PCIE-32GB           On  |   00000000:5E:00.0 Off |                    0 |\r\n",
      "| N/A   33C    P0             25W /  250W |       1MiB /  32768MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "|   1  Tesla V100-PCIE-32GB           On  |   00000000:AF:00.0 Off |                    0 |\r\n",
      "| N/A   38C    P0             27W /  250W |       1MiB /  32768MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef27f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starfdgted\n"
     ]
    }
   ],
   "source": [
    "print(\"starfdgted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb37039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e47840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/bpopper3/.conda/envs/vlm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Label distribution in training set: {0: 12500, 1: 12500}\n",
      "Label distribution in testing set: {0: 2494, 1: 2506}\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/bpopper3/.conda/envs/vlm/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbrieucpopper\u001b[0m (\u001b[33mbrieuc_popper\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hice1/bpopper3/ondemand/dl_ft/wandb/run-20241201_200810-pvcb2anq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/brieuc_popper/huggingface/runs/pvcb2anq' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/brieuc_popper/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/brieuc_popper/huggingface' target=\"_blank\">https://wandb.ai/brieuc_popper/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/brieuc_popper/huggingface/runs/pvcb2anq' target=\"_blank\">https://wandb.ai/brieuc_popper/huggingface/runs/pvcb2anq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "/home/hice1/bpopper3/.conda/envs/vlm/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='874' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [874/980 54:32 < 06:37, 0.27 it/s, Epoch 8.91/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.600100</td>\n",
       "      <td>0.561242</td>\n",
       "      <td>0.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.412877</td>\n",
       "      <td>0.810800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.339269</td>\n",
       "      <td>0.856200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.311763</td>\n",
       "      <td>0.874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.304194</td>\n",
       "      <td>0.874800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.297464</td>\n",
       "      <td>0.874600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.273300</td>\n",
       "      <td>0.279658</td>\n",
       "      <td>0.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.286300</td>\n",
       "      <td>0.269417</td>\n",
       "      <td>0.891000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at ./custom_checkpoints_run2/epoch-1\n",
      "Saving model at ./custom_checkpoints_run2/epoch-2\n",
      "Saving model at ./custom_checkpoints_run2/epoch-3\n",
      "Saving model at ./custom_checkpoints_run2/epoch-4\n",
      "Saving model at ./custom_checkpoints_run2/epoch-5\n",
      "Saving model at ./custom_checkpoints_run2/epoch-6\n",
      "Saving model at ./custom_checkpoints_run2/epoch-7\n",
      "Saving model at ./custom_checkpoints_run2/epoch-8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "# Custom Sentiment Classification Model\n",
    "class SentimentClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        # Freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Get the dimension of the base model's last hidden state\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        \n",
    "        # Remove existing classification head\n",
    "        if hasattr(base_model, 'score'):\n",
    "            delattr(base_model, 'score')\n",
    "        \n",
    "        # Add new classification layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 2)  # 2 neurons for binary sentiment\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get base model outputs\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "    \n",
    "        hidden_state = outputs.hidden_states[-1][:, -2, :]\n",
    "        #Last sentence is \"This movie review is\" so -1 is \" is\" and -2 is \" review\"\n",
    "        \n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(hidden_state)\n",
    "        \n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits\n",
    "        }\n",
    "    def save_pretrained(self, save_path):\n",
    "        # Only save the trainable classifier weights and configuration\n",
    "        torch.save({\n",
    "            'classifier_state_dict': self.classifier.state_dict(),\n",
    "            'hidden_size': self.base_model.config.hidden_size\n",
    "        }, save_path)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, base_model, load_path):\n",
    "        # Recreate the model with the base model\n",
    "        model = cls(base_model)\n",
    "        \n",
    "        # Load the saved state\n",
    "        checkpoint = torch.load(load_path)\n",
    "        \n",
    "        # Ensure the hidden size matches\n",
    "        assert checkpoint['hidden_size'] == base_model.config.hidden_size, \\\n",
    "            \"Loaded model's hidden size does not match the base model\"\n",
    "        \n",
    "        # Load only the classifier weights\n",
    "        model.classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
    "        \n",
    "        return model\n",
    "import os\n",
    "\n",
    "class SaveModelCallback(TrainerCallback):\n",
    "    def __init__(self, model, save_dir=\"custom_checkpoints\"):\n",
    "        self.model = model\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Define the save path for the current epoch\n",
    "        save_path = os.path.join(self.save_dir, f\"epoch-{state.epoch:.0f}\")\n",
    "        print(f\"Saving model at {save_path}\")\n",
    "        \n",
    "        # Save the model\n",
    "        self.model.save_pretrained(save_path)\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# Wrap base model in custom sentiment classification model\n",
    "model = SentimentClassificationModel(base_model).to(device)\n",
    "\n",
    "# Load IMDB dataset\n",
    "def prepare_dataset():\n",
    "    # Load dataset\n",
    "    # Shuffle dataset to ensure diversity\n",
    "    dataset = load_dataset('imdb')\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    \n",
    "    # Reduce dataset size if needed\n",
    "    train_size = int(1 * len(dataset['train']))\n",
    "    test_size = int(0.2 * len(dataset['test']))\n",
    "    dataset['train'] = dataset['train'].select(range(train_size))\n",
    "    dataset['test'] = dataset['test'].select(range(test_size))\n",
    "    dataset['unsupervised'] = dataset['unsupervised'].select(range(1)) # should not be used\n",
    "    \n",
    "    # Check label distribution before tokenization\n",
    "    train_labels = [example['label'] for example in dataset['train']]\n",
    "    test_labels = [example['label'] for example in dataset['test']]\n",
    "    print(f\"Label distribution in training set: {dict((x, train_labels.count(x)) for x in set(train_labels))}\")\n",
    "    print(f\"Label distribution in testing set: {dict((x, test_labels.count(x)) for x in set(test_labels))}\")\n",
    "    \n",
    "   \n",
    "    def modify_text(t):\n",
    "        \n",
    "        t = t[:2200] # approx. ensures that this fits in 768 tokens max\n",
    "        return f'\"I loved this ! Great actors\" This movie review is positive. \"The ending was a bit disappointing. Also hard to understand.\" This movie review is negative. \"{t}\" This movie review is'\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "\n",
    "        return tokenizer(\n",
    "            [modify_text(t) for t in examples['text']],\n",
    "            truncation=True, \n",
    "            max_length=768\n",
    "        )\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "    \n",
    "    # Rename label column to match Trainer's expectation\n",
    "    tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "\n",
    "    #print sample example from train and label\n",
    "    print(tokenized_datasets['train'][4]['labels'])\n",
    "    print(tokenized_datasets['train'][5]['labels'])\n",
    "    print(tokenized_datasets['train'][6]['labels'])\n",
    "    print(tokenized_datasets['train'][7]['labels'])\n",
    "    print(tokenized_datasets['train'][8]['labels'])\n",
    "\n",
    "    return tokenized_datasets\n",
    "\n",
    "\n",
    "\n",
    "# Prepare datasets\n",
    "tokenized_datasets = prepare_dataset()\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10, \n",
    "    per_device_train_batch_size=128, #adjust based on GPU memory and max_length\n",
    "    per_device_eval_batch_size=128, #128 seems good for 32 VRAM and max size 768\n",
    "    warmup_steps=120,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Simple accuracy calculation\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Define the save directory\n",
    "save_dir = \"./custom_checkpoints_run2\"\n",
    "\n",
    "# Create the callback\n",
    "save_callback = SaveModelCallback(model, save_dir=save_dir)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[save_callback]\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "#Train the model\n",
    "\n",
    "DO_TRAIN = True\n",
    "if DO_TRAIN:\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning complete!\")\n",
    "    #save\n",
    "    model.save_pretrained(\"sentiment_model_run6803_1_12\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661241f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from /home/bpopper/gtCode/DL-Final-Project/code/finetune/sentiment_model#\n",
    "print('Loading model from ckpt')\n",
    "model = SentimentClassificationModel.from_pretrained(base_model, \"./custom_checkpoints/epoch-1\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Inference function\n",
    "def predict_sentiment(text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.softmax(outputs['logits'], dim=1)\n",
    "        sentiment = torch.argmax(predictions, dim=1)\n",
    "        print(predictions)\n",
    "    \n",
    "    return \"Positive\" if sentiment.item() == 1 else \"Negative\"\n",
    "\n",
    "# Example usage\n",
    "example_text = \"This movie was absolutely fantastic and I loved every minute of it!\"\n",
    "print(f\"Sentiment: {predict_sentiment(example_text)}\")\n",
    "\n",
    "\n",
    "\n",
    "example_text2 = \"Th fucking bad!\"\n",
    "print(f\"Sentiment: {predict_sentiment(example_text2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3cb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752da73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-vlm]",
   "language": "python",
   "name": "conda-env-.conda-vlm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
