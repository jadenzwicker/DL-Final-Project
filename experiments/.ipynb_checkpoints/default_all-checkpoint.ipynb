{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and setup Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pace Setup\n",
    "# !module load anaconda3\n",
    "# !module load gcc/12.3.0\n",
    "# !module load cuda/12.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec  7 22:01:32 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           On  |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   47C    P0             27W /  250W |       1MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/3/5/jzwicker3/dl-fp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BatchEncoding' from 'transformers' (/storage/ice1/3/5/jzwicker3/dl-fp/lib/python3.12/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchEncoding\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprettytable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrettyTable\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Update HF cache directory\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BatchEncoding' from 'transformers' (/storage/ice1/3/5/jzwicker3/dl-fp/lib/python3.12/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "# from transformers import BatchEncoding\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Update HF cache directory\n",
    "env_path = os.path.abspath(os.path.join(os.getcwd(), '..', '.env'))\n",
    "print(env_path)\n",
    "load_dotenv(env_path)\n",
    "hf_cache_dir = os.getenv('TRANSFORMERS_CACHE')\n",
    "os.makedirs(hf_cache_dir, exist_ok=True)\n",
    "print(f\"Hugging Face cache directory set to: {hf_cache_dir}\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check cuda version torch is using\n",
    "print(f\"Using torch {torch.__version__} with cuda {torch.version.cuda}\")\n",
    "\n",
    "workspace_dir = os.getenv('WORKSPACE_DIR')\n",
    "\n",
    "seed = 42\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset\n",
    "It is stored in the dataset directory which is gitignored so run this block to repopulate if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if dataset is present\n",
    "# dataset_dir = os.path.join(workspace_dir, 'datasets')\n",
    "# os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# if not os.path.exists(os.path.join(dataset_dir, 'IMDB Dataset.csv')):\n",
    "#     !kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews --path {dataset_dir} --unzip\n",
    "\n",
    "# # Load dataset into dataframe\n",
    "# dataset = pd.read_csv(os.path.join(dataset_dir, 'IMDB Dataset.csv'))\n",
    "# print(dataset.head())\n",
    "\n",
    "# _, test_set = train_test_split(dataset, test_size=0.2, random_state=seed)\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "test_dataset = imdb['test'].shuffle(seed=seed).select([i for i in list(range(500))])\n",
    "train_dataset = imdb['train'].shuffle(seed=seed)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To calculate the values of accuracy, recall, specificity, precision, and F-score, you need the confusion matrix or the key components: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). Here's how each metric is calculated:\n",
    "\n",
    "1. **Accuracy**: The proportion of correctly classified instances (both positive and negative) out of all instances.\n",
    "   \\[\n",
    "   \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "   \\]\n",
    "\n",
    "2. **Recall (Sensitivity)**: The proportion of actual positives correctly identified.\n",
    "   \\[\n",
    "   \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "   \\]\n",
    "\n",
    "3. **Specificity**: The proportion of actual negatives correctly identified.\n",
    "   \\[\n",
    "   \\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "   \\]\n",
    "\n",
    "4. **Precision**: The proportion of predicted positives that are actually positive.\n",
    "   \\[\n",
    "   \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "   \\]\n",
    "\n",
    "5. **F-score**: The harmonic mean of precision and recall, balancing the two.\n",
    "   \\[\n",
    "   \\text{F-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   \\]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def evaluate_model_zero_shot(model, tokenizer, device, dataset, top_k=50):\n",
    "    # Data preparation\n",
    "    reviews = [example['text'] for example in dataset]\n",
    "    true_labels = [example['label'] for example in dataset]  # 0 for negative, 1 for positive\n",
    "\n",
    "    # Define the prompts\n",
    "    prompts = [f\"Movie Review: {review} \\n Only Answer if this Movie Review is Positive or Negative:\" for review in reviews]\n",
    "\n",
    "    # Perform inference\n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "\n",
    "    for idx, example in tqdm(enumerate(dataset), total=len(dataset), desc=\"Processing\", leave=True):\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer.encode(prompts[idx], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Get the probabilities for the next token\n",
    "        next_token_logits = logits[:, -1, :]  # Only consider the last token's logits\n",
    "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Get the top k most likely tokens\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k) # these are sorted in order of most likely to least likely\n",
    "\n",
    "        # Decode the top k tokens\n",
    "        top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices[0]]\n",
    "\n",
    "        # Extract the sentiment prediction from the top k tokens, if the model did not predict a sentiment, default to negative\n",
    "        pred = -1\n",
    "        for token in top_k_tokens:\n",
    "            token_lower = token.strip().lower()\n",
    "            if token_lower == 'positive':\n",
    "                pred = 1\n",
    "                break\n",
    "            elif token_lower == 'negative':\n",
    "                pred = 0\n",
    "            \n",
    "        if pred == -1:\n",
    "            print(f\"Could not predict sentiment for review: {reviews[idx]}\")\n",
    "            print(f\"Top k tokens: {top_k_tokens}\")\n",
    "            pred = 0\n",
    "\n",
    "        # If the model did not predict a sentiment, default to negative\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "\n",
    "    # Calculate confusion matrix    \n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Calculate total and average inference times\n",
    "    total_inference_time = sum(inference_times)\n",
    "    average_inference_time = total_inference_time / len(inference_times)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"precision\": precision,\n",
    "        \"f_score\": f_score,\n",
    "        \"total_inference_time\": total_inference_time,\n",
    "        \"average_inference_time\": average_inference_time\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_few_shot(model, tokenizer, device, dataset, top_k=50):\n",
    "    # Data preparation\n",
    "    reviews = [example['text'] for example in dataset]\n",
    "    true_labels = [example['label'] for example in dataset]  # 0 for negative, 1 for positive\n",
    "\n",
    "    # Define the prompts\n",
    "    # example_review = \"This movie was an absolute masterpiece with stunning visuals and a gripping story!\"\n",
    "    # example_review_neg = \"This movie was terrible and I hated it.\"\n",
    "    # example_negative_review_2 = \"I really think this movie is not that good. It was a waste of time.\"\n",
    "    few_shot_rev_1 = \"Movie Review: I loved this movie ! So good plot ! \\n Only Answer if this Movie Review is Positive or Negative: Positive \\n\"\n",
    "    few_shot_rev_2 = \"Movie Review: I hated this, could be a lot better \\n Only Answer if this Movie Review is Positive or Negative: Negative \\n\"\n",
    " \n",
    "    prompts = [f\"{few_shot_rev_1} {few_shot_rev_2} Movie Review: {review} \\n Only Answer if this Movie Review is Positive or Negative:\" for review in reviews]\n",
    "\n",
    "    # Perform inference\n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "\n",
    "    for idx, example in tqdm(enumerate(dataset), total=len(dataset), desc=\"Processing\", leave=True):\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer.encode(prompts[idx], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Get the probabilities for the next token\n",
    "        next_token_logits = logits[:, -1, :]  # Only consider the last token's logits\n",
    "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Get the top k most likely tokens\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k) # these are sorted in order of most likely to least likely\n",
    "\n",
    "        # Decode the top k tokens\n",
    "        top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices[0]]\n",
    "\n",
    "        # Extract the sentiment prediction from the top k tokens, if the model did not predict a sentiment, default to negative\n",
    "        pred = -1\n",
    "        for token in top_k_tokens:\n",
    "            token_lower = token.strip().lower()\n",
    "            if token_lower == 'positive':\n",
    "                pred = 1\n",
    "                break\n",
    "            elif token_lower == 'negative':\n",
    "                pred = 0\n",
    "            \n",
    "        if pred == -1:\n",
    "            print(f\"Could not predict sentiment for review: {reviews[idx]}\")\n",
    "            print(f\"Top k tokens: {top_k_tokens}\")\n",
    "            pred = 0\n",
    "\n",
    "        # If the model did not predict a sentiment, default to negative\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "    # Calculate confusion matrix    \n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Calculate total and average inference times\n",
    "    total_inference_time = sum(inference_times)\n",
    "    average_inference_time = total_inference_time / len(inference_times)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"precision\": precision,\n",
    "        \"f_score\": f_score,\n",
    "        \"total_inference_time\": total_inference_time,\n",
    "        \"average_inference_time\": average_inference_time\n",
    "    }\n",
    "\n",
    "\n",
    "def create_results_table(results_dict, model_name=\"Model Results\"):\n",
    "    \"\"\"\n",
    "    Creates a formatted table from the results dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "        results_dict (dict): Dictionary containing evaluation metrics.\n",
    "        model_name (str): Name of the model being evaluated.\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted table as a string.\n",
    "    \"\"\"\n",
    "    # Initialize a PrettyTable\n",
    "    table = PrettyTable()\n",
    "    \n",
    "    # Set the table title\n",
    "    table.title = f\"Results for {model_name}\"\n",
    "    \n",
    "    # Add columns\n",
    "    table.field_names = [\"Metric\", \"Value\"]\n",
    "    \n",
    "    # Add rows for each metric\n",
    "    table.add_row([\"Accuracy\", f\"{results_dict['accuracy']:.2f}\"])\n",
    "    table.add_row([\"Recall (Sensitivity)\", f\"{results_dict['recall']:.2f}\"])\n",
    "    if \"specificity\" in results_dict:  # Specificity might not be included in some results\n",
    "        table.add_row([\"Specificity\", f\"{results_dict['specificity']:.2f}\"])\n",
    "    table.add_row([\"Precision\", f\"{results_dict['precision']:.2f}\"])\n",
    "    table.add_row([\"F-Score\", f\"{results_dict['f_score']:.2f}\"])\n",
    "    table.add_row([\"Total Inference Time (s)\", f\"{results_dict['total_inference_time']:.2f}\"])\n",
    "    table.add_row([\"Average Inference Time (s)\", f\"{results_dict['average_inference_time']:.2f}\"])\n",
    "    \n",
    "    # Return the table as a string\n",
    "    return table.get_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLM-135M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears cuda from last run\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "#FROM https://huggingface.co/HuggingFaceTB/SmolLM2-135M TODO: Dont forget to cite the model in report\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_135_results = evaluate_model_zero_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "zero_135_table = create_results_table(zero_135_results, model_name=\"SmolLM2-135M Zero-Shot\")\n",
    "print(zero_135_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_135_results = evaluate_model_few_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "few_135_table = create_results_table(few_135_results, model_name=\"SmolLM2-135M Few-Shot\")\n",
    "print(few_135_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLM-360M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears cuda from last run\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_360_results = evaluate_model_zero_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "zero_360_table = create_results_table(zero_360_results, model_name=\"SmolLM2-360M Zero-Shot\")\n",
    "print(zero_360_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_360_results = evaluate_model_few_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "few_360_table = create_results_table(few_360_results, model_name=\"SmolLM2-360M Few-Shot\")\n",
    "print(few_360_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLM2-1.7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clears cuda from last run\n",
    "# if device == \"cuda\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     torch.cuda.ipc_collect()\n",
    "#     torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_17_results = evaluate_model_zero_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "zero_17_table = create_results_table(zero_17_results, model_name=\"SmolLM2-1.7B Zero-Shot\")\n",
    "print(zero_17_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_17_results = evaluate_model_few_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "few_17_table = create_results_table(few_17_results, model_name=\"SmolLM2-1.7B Few-Shot\")\n",
    "print(few_17_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All models evaluated successfully!\")\n",
    "\n",
    "results_dicts = [\n",
    "    zero_135_results,\n",
    "    few_135_results,\n",
    "    zero_360_results,\n",
    "    few_360_results,\n",
    "    zero_17_results,\n",
    "    few_17_results,\n",
    "]\n",
    "\n",
    "# List of model names\n",
    "model_names = [\n",
    "    \"SmolLM2-135M Zero-Shot\",\n",
    "    \"SmolLM2-135M Few-Shot\",\n",
    "    \"SmolLM2-360M Zero-Shot\",\n",
    "    \"SmolLM2-360M Few-Shot\",\n",
    "    \"SmolLM2-1.7B Zero-Shot\",\n",
    "    \"SmolLM2-1.7B Few-Shot\",\n",
    "]\n",
    "\n",
    "table = PrettyTable()\n",
    "\n",
    "# Define the columns\n",
    "table.field_names = [\n",
    "    \"Model\",\n",
    "    \"Accuracy\",\n",
    "    \"Recall (Sensitivity)\",\n",
    "    \"Specificity\",\n",
    "    \"Precision\",\n",
    "    \"F-Score\",\n",
    "    \"Total Inference Time (s)\",\n",
    "    \"Avg Inference Time (s)\"\n",
    "]\n",
    "\n",
    "# Populate the table\n",
    "for model_name, results in zip(model_names, results_dicts):\n",
    "    table.add_row([\n",
    "        model_name,\n",
    "        f\"{results['accuracy']:.2f}\",\n",
    "        f\"{results['recall']:.2f}\",\n",
    "        f\"{results['specificity']:.2f}\",\n",
    "        f\"{results['precision']:.2f}\",\n",
    "        f\"{results['f_score']:.2f}\",\n",
    "        f\"{results['total_inference_time']:.2f}\",\n",
    "        f\"{results['average_inference_time']:.2f}\",\n",
    "    ])\n",
    "\n",
    "print(table)\n",
    "print(results_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears cuda from last run\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl-fp]",
   "language": "python",
   "name": "conda-env-dl-fp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
