{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and setup Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face cache directory set to: /root/repos/DL-Final-Project/.cache/huggingface\n",
      "Using torch 2.5.1 with cuda 12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl-fp-env/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "# Update HF cache directory\n",
    "env_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '.env'))\n",
    "load_dotenv(env_path)\n",
    "hf_cache_dir = os.getenv('TRANSFORMERS_CACHE')\n",
    "os.makedirs(hf_cache_dir, exist_ok=True)\n",
    "print(f\"Hugging Face cache directory set to: {hf_cache_dir}\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check cuda version torch is using\n",
    "print(f\"Using torch {torch.__version__} with cuda {torch.version.cuda}\")\n",
    "\n",
    "workspace_dir = os.getenv('WORKSPACE_DIR')\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset\n",
    "It is stored in the dataset directory which is gitignored so run this block to repopulate if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 25000\n",
      "Test dataset: 1000\n",
      "{'text': \"<br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br />Keep up the good work guys, you CAN and DO make a difference.<br /><br />\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# # Check if dataset is present\n",
    "# dataset_dir = os.path.join(workspace_dir, 'datasets')\n",
    "# os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# if not os.path.exists(os.path.join(dataset_dir, 'IMDB Dataset.csv')):\n",
    "#     !kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews --path {dataset_dir} --unzip\n",
    "\n",
    "# # Load dataset into dataframe\n",
    "# dataset = pd.read_csv(os.path.join(dataset_dir, 'IMDB Dataset.csv'))\n",
    "# print(dataset.head())\n",
    "\n",
    "# _, test_set = train_test_split(dataset, test_size=0.2, random_state=seed)\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "test_dataset = imdb['test'].shuffle(seed=seed).select([i for i in list(range(1000))])\n",
    "train_dataset = imdb['train'].shuffle(seed=seed)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Experiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define the movie review for classification\n",
    "example_review = \"This movie was an absolute masterpiece with stunning visuals and a gripping story!\"\n",
    "example_review_neg = \"This movie was terrible and I hated it.\"\n",
    "example_negative_review_2 = \"I really think this movie is not that good. It was a waste of time.\"\n",
    "\n",
    "example_inference_review = \"what a movie ! changed my life! I love luke's character and actor\"\n",
    "# Prompt for zero-shot classification\n",
    "prompt = [\n",
    "    ['Review :', example_review, ' Sentiment[positive/negative] :', ' positive','\\n'],\n",
    "    ['Review :', example_review_neg, ' Sentiment[positive/negative] :', ' negative','\\n'],\n",
    "    ['Review :', example_negative_review_2, ' Sentiment[positive/negative] :', ' negative','\\n'],\n",
    "    ['Review :', example_inference_review, ' Sentiment[positive/negative] :'],\n",
    "]\n",
    "\n",
    "prompt_=[]\n",
    "for i in prompt:\n",
    "    prompt_.append(''.join(i))\n",
    "#make all attached\n",
    "prompt = ''.join(prompt_)\n",
    "\n",
    "# Tokenize and set up for inference\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the output\n",
    "start_inf_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=5)\n",
    "\n",
    "end_inf_time = time.time()\n",
    "\n",
    "# Decode and print the output\n",
    "output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_str)\n",
    "\n",
    "print(f\"Inference time: {end_inf_time - start_inf_time:.2f} seconds\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "def test_model(few_shot_prompt,test_dataset):                       \n",
    "    total_right = 0\n",
    "    \n",
    "    for entry in tqdm(test_dataset):\n",
    "        rev = entry['text']\n",
    "        posNeg = entry['label']\n",
    "        input_text = f\"{few_shot_prompt}\" + f'\"{rev}\" This movie review is'\n",
    "        \n",
    "    \n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Get the probabilities for the next token\n",
    "        next_token_logits = logits[:, -1, :]  # Only consider the last token's logits\n",
    "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Get the top 10 most likely tokens\n",
    "        top_k = 10\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
    "\n",
    "        # Decode the top 10 tokens\n",
    "        top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices[0]]\n",
    "        pred = 1\n",
    "        for tok in top_k_tokens:\n",
    "            if tok == ' positive':\n",
    "                #print('pssssss')\n",
    "                pred = 1\n",
    "                break\n",
    "            elif tok == ' negative':\n",
    "                #print('nggg')\n",
    "                pred = 0\n",
    "                break\n",
    "        if pred == posNeg:\n",
    "            total_right+=1\n",
    "        \n",
    "        \n",
    "#         print(rev)\n",
    "#         print(f\"pred is {pred}\")\n",
    "#         print(f\"the target was {posNeg}\")\n",
    "#         print('\\n')\n",
    "        \n",
    "    print(total_right/len(test_dataset))\n",
    "\n",
    "\n",
    "test_model(few_shot_prompt,small_test_dataset)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_zero_shot(model, tokenizer, device, dataset, top_k=50):\n",
    "    # Data preparation\n",
    "    reviews = [example['text'] for example in dataset]\n",
    "    true_labels = [example['label'] for example in dataset]  # 0 for negative, 1 for positive\n",
    "\n",
    "    # Define the prompts\n",
    "    prompts = [f\"Movie Review: {review} \\n Only Answer if this Movie Review is Positive or Negative:\" for review in reviews]\n",
    "\n",
    "    # Perform inference\n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "\n",
    "    for idx, example in tqdm(enumerate(dataset), total=len(dataset), desc=\"Processing\", leave=True):\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer.encode(prompts[idx], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Get the probabilities for the next token\n",
    "        next_token_logits = logits[:, -1, :]  # Only consider the last token's logits\n",
    "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Get the top k most likely tokens\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k) # these are sorted in order of most likely to least likely\n",
    "\n",
    "        # Decode the top k tokens\n",
    "        top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices[0]]\n",
    "\n",
    "        # Extract the sentiment prediction from the top 10 tokens\n",
    "        pred = -1\n",
    "        for token in top_k_tokens:\n",
    "            token_lower = token.strip().lower()\n",
    "            if token_lower == 'positive':\n",
    "                pred = 1\n",
    "                break\n",
    "            elif token_lower == 'negative':\n",
    "                pred = 0\n",
    "                break\n",
    "\n",
    "        # If the model did not predict a sentiment, default to negative\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    total_inference_time = sum(inference_times)\n",
    "    average_inference_time = total_inference_time / len(inference_times)\n",
    "    return accuracy, total_inference_time, average_inference_time\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_few_shot(model, tokenizer, device, dataset, top_k=50):\n",
    "    # Data preparation\n",
    "    reviews = [example['text'] for example in dataset]\n",
    "    true_labels = [example['label'] for example in dataset]  # 0 for negative, 1 for positive\n",
    "\n",
    "    # Define the prompts\n",
    "    # example_review = \"This movie was an absolute masterpiece with stunning visuals and a gripping story!\"\n",
    "    # example_review_neg = \"This movie was terrible and I hated it.\"\n",
    "    # example_negative_review_2 = \"I really think this movie is not that good. It was a waste of time.\"\n",
    "    few_shot_rev_1 = \"Movie Review: I loved this movie ! So good plot ! \\n Only Answer if this Movie Review is Positive or Negative: Positive \\n\"\n",
    "    few_shot_rev_2 = \"Movie Review: I hated this, could be a lot better \\n Only Answer if this Movie Review is Positive or Negative: Negative \\n\"\n",
    " \n",
    "    prompts = [f\"{few_shot_rev_1} {few_shot_rev_2} Movie Review: {review} \\n Only Answer if this Movie Review is Positive or Negative:\" for review in reviews]\n",
    "\n",
    "    # Perform inference\n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "\n",
    "    for idx, example in tqdm(enumerate(dataset), total=len(dataset), desc=\"Processing\", leave=True):\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer.encode(prompts[idx], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Get the probabilities for the next token\n",
    "        next_token_logits = logits[:, -1, :]  # Only consider the last token's logits\n",
    "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Get the top k most likely tokens\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k) # these are sorted in order of most likely to least likely\n",
    "\n",
    "        # Decode the top k tokens\n",
    "        top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices[0]]\n",
    "\n",
    "        # Extract the sentiment prediction from the top 10 tokens\n",
    "        pred = -1\n",
    "        for token in top_k_tokens:\n",
    "            token_lower = token.strip().lower()\n",
    "            if token_lower == 'positive':\n",
    "                pred = 1\n",
    "                break\n",
    "            elif token_lower == 'negative':\n",
    "                pred = 0\n",
    "                break\n",
    "\n",
    "        # If the model did not predict a sentiment, default to negative\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    total_inference_time = sum(inference_times)\n",
    "    average_inference_time = total_inference_time / len(inference_times)\n",
    "    return accuracy, total_inference_time, average_inference_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLM-135M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "# Clears cuda from last run\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "#FROM https://huggingface.co/HuggingFaceTB/SmolLM2-135M TODO: Dont forget to cite the model in report\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a77d9017084388ab9d89e441b084c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44\n",
      "Total inference time: 15.33 seconds\n",
      "Average inference time: 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "zero_135_accuracy, zero_135_total_time, zero_135_average_time = evaluate_model_zero_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "print(f\"Accuracy: {zero_135_accuracy:.2f}\")\n",
    "print(f\"Total inference time: {zero_135_total_time:.2f} seconds\")\n",
    "print(f\"Average inference time: {zero_135_average_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d921c36c218c4450a76f6a7585bed87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49\n",
      "Total inference time: 16.97 seconds\n",
      "Average inference time: 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "few_135_accuracy, few_135_total_time, few_135_average_time = evaluate_model_few_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "print(f\"Accuracy: {few_135_accuracy:.2f}\")\n",
    "print(f\"Total inference time: {few_135_total_time:.2f} seconds\")\n",
    "print(f\"Average inference time: {few_135_average_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears cuda from last run\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
