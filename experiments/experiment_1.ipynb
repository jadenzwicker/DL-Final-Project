{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and setup Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face cache directory set to: /root/repos/DL-Final-Project/.cache/huggingface\n",
      "Using torch 2.5.1 with cuda 12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl-fp-env/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Update HF cache directory\n",
    "env_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '.env'))\n",
    "load_dotenv(env_path)\n",
    "hf_cache_dir = os.getenv('TRANSFORMERS_CACHE')\n",
    "os.makedirs(hf_cache_dir, exist_ok=True)\n",
    "print(f\"Hugging Face cache directory set to: {hf_cache_dir}\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check cuda version torch is using\n",
    "print(f\"Using torch {torch.__version__} with cuda {torch.version.cuda}\")\n",
    "\n",
    "workspace_dir = os.getenv('WORKSPACE_DIR')\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset\n",
    "It is stored in the dataset directory which is gitignored so run this block to repopulate if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 25000\n",
      "Test dataset: 25000\n",
      "{'text': \"<br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br />Keep up the good work guys, you CAN and DO make a difference.<br /><br />\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# # Check if dataset is present\n",
    "# dataset_dir = os.path.join(workspace_dir, 'datasets')\n",
    "# os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# if not os.path.exists(os.path.join(dataset_dir, 'IMDB Dataset.csv')):\n",
    "#     !kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews --path {dataset_dir} --unzip\n",
    "\n",
    "# # Load dataset into dataframe\n",
    "# dataset = pd.read_csv(os.path.join(dataset_dir, 'IMDB Dataset.csv'))\n",
    "# print(dataset.head())\n",
    "\n",
    "# _, test_set = train_test_split(dataset, test_size=0.2, random_state=seed)\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "test_dataset = imdb['test'].shuffle(seed=seed) #.select([i for i in list(range(500))])\n",
    "train_dataset = imdb['train'].shuffle(seed=seed)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup models and compare to literature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FROM https://huggingface.co/HuggingFaceTB/SmolLM-135M TODO: Dont forget to cite the model in report\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M\"    \n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|██████████| 25000/25000 [10:31<00:00, 39.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.52\n",
      "Average inference time per review: 0.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Define the movie review for classification\n",
    "example_review = \"This movie was an absolute masterpiece with stunning visuals and a gripping story!\"\n",
    "example_review_neg = \"This movie was terrible and I hated it.\"\n",
    "example_negative_review_2 = \"I really think this movie is not that good. It was a waste of time.\"\n",
    "\n",
    "example_inference_review = \"what a movie ! changed my life! I love luke's character and actor\"\n",
    "# Prompt for zero-shot classification\n",
    "prompt = [\n",
    "    ['Review :', example_review, ' Sentiment[positive/negative] :', ' positive','\\n'],\n",
    "    ['Review :', example_review_neg, ' Sentiment[positive/negative] :', ' negative','\\n'],\n",
    "    ['Review :', example_negative_review_2, ' Sentiment[positive/negative] :', ' negative','\\n'],\n",
    "    ['Review :', example_inference_review, ' Sentiment[positive/negative] :'],\n",
    "]\n",
    "\n",
    "prompt_=[]\n",
    "for i in prompt:\n",
    "    prompt_.append(''.join(i))\n",
    "#make all attached\n",
    "prompt = ''.join(prompt_)\n",
    "\n",
    "# Tokenize and set up for inference\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate the output\n",
    "start_inf_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=5)\n",
    "\n",
    "end_inf_time = time.time()\n",
    "\n",
    "# Decode and print the output\n",
    "output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_str)\n",
    "\n",
    "print(f\"Inference time: {end_inf_time - start_inf_time:.2f} seconds\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "def test_model(few_shot_prompt,test_dataset):                       \n",
    "    total_right = 0\n",
    "    \n",
    "    for entry in tqdm(test_dataset):\n",
    "        rev = entry['text']\n",
    "        posNeg = entry['label']\n",
    "        input_text = f\"{few_shot_prompt}\" + f'\"{rev}\" This movie review is'\n",
    "        \n",
    "    \n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Get the probabilities for the next token\n",
    "        next_token_logits = logits[:, -1, :]  # Only consider the last token's logits\n",
    "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Get the top 10 most likely tokens\n",
    "        top_k = 10\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
    "\n",
    "        # Decode the top 10 tokens\n",
    "        top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices[0]]\n",
    "        pred = 1\n",
    "        for tok in top_k_tokens:\n",
    "            if tok == ' positive':\n",
    "                #print('pssssss')\n",
    "                pred = 1\n",
    "                break\n",
    "            elif tok == ' negative':\n",
    "                #print('nggg')\n",
    "                pred = 0\n",
    "                break\n",
    "        if pred == posNeg:\n",
    "            total_right+=1\n",
    "        \n",
    "        \n",
    "#         print(rev)\n",
    "#         print(f\"pred is {pred}\")\n",
    "#         print(f\"the target was {posNeg}\")\n",
    "#         print('\\n')\n",
    "        \n",
    "    print(total_right/len(test_dataset))\n",
    "\n",
    "\n",
    "test_model(few_shot_prompt,small_test_dataset)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = []\n",
    "true_labels = []\n",
    "inference_times = []\n",
    "\n",
    "for example in tqdm(test_dataset):\n",
    "    review = example['text']\n",
    "    true_label = example['label']  # 0 for negative, 1 for positive\n",
    "    true_labels.append(true_label)\n",
    "\n",
    "    # Define the prompt\n",
    "    prompt = f\"Movie Review: {review} \\n Only Answer if this Movie Review is Positive or Negative:\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Get the probabilities for the next token\n",
    "    next_token_logits = logits[:, -1, :]  # Only consider the last token's logits\n",
    "    probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Get the top 10 most likely tokens\n",
    "    top_k = 50\n",
    "    top_k_probs, top_k_indices = torch.topk(probabilities, top_k) # these are sorted in order of most likely to least likely\n",
    "\n",
    "    # Decode the top 10 tokens\n",
    "    top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices[0]]\n",
    "\n",
    "    # print(top_k_tokens)\n",
    "\n",
    "    # Extract the sentiment prediction from the top 10 tokens\n",
    "    pred = -1\n",
    "    for token in top_k_tokens:\n",
    "        token_lower = token.strip().lower()\n",
    "        if token_lower == 'positive':\n",
    "            pred = 1\n",
    "            break\n",
    "        elif token_lower == 'negative':\n",
    "            pred = 0\n",
    "            break\n",
    "\n",
    "    # If the model did not predict a sentiment, default to negative\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    inference_times.append(end_time - start_time)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Average inference time per review: {sum(inference_times) / len(inference_times):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [11:13<00:00, 37.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n",
      "Average inference time per review: 0.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "predictions = []\n",
    "true_labels = []\n",
    "inference_times = []\n",
    "\n",
    "for example in tqdm(test_dataset):\n",
    "    review = example['text']\n",
    "    true_label = example['label']  # 0 for negative, 1 for positive\n",
    "    true_labels.append(true_label)\n",
    "\n",
    "    # Define the prompt\n",
    "    example_review = \"This movie was an absolute masterpiece with stunning visuals and a gripping story!\"\n",
    "    example_review_neg = \"This movie was terrible and I hated it.\"\n",
    "    example_negative_review_2 = \"I really think this movie is not that good. It was a waste of time.\"\n",
    "    few_shot_rev_1 = \"Movie Review: I loved this movie ! So good plot ! \\n Only Answer if this Movie Review is Positive or Negative: Positive\"\n",
    "    few_shot_rev_2 = \"I hated this, could be a lot better \\n Only Answer if this Movie Review is Positive or Negative: Negative\"\n",
    "    prompt = f\"Movie Review : {review} \\n Only Answer if this Movie Review is Positive or Negative:\"\n",
    "\n",
    "    combined_prompt = f\"{few_shot_rev_1} {few_shot_rev_2} {prompt}\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode(combined_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Get the probabilities for the next token\n",
    "    next_token_logits = logits[:, -1, :]  # Only consider the last token's logits\n",
    "    probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Get the top 10 most likely tokens\n",
    "    top_k = 50\n",
    "    top_k_probs, top_k_indices = torch.topk(probabilities, top_k) # these are sorted in order of most likely to least likely\n",
    "\n",
    "    # Decode the top 10 tokens\n",
    "    top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices[0]]\n",
    "\n",
    "    # print(top_k_tokens)\n",
    "\n",
    "    # Extract the sentiment prediction from the top 10 tokens\n",
    "    pred = -1\n",
    "    for token in top_k_tokens:\n",
    "        token_lower = token.strip().lower()\n",
    "        if token_lower == 'positive':\n",
    "            pred = 1\n",
    "            break\n",
    "        elif token_lower == 'negative':\n",
    "            pred = 0\n",
    "            break\n",
    "\n",
    "    # If the model did not predict a sentiment, default to negative\n",
    "    predictions.append(pred)\n",
    "    \n",
    "    inference_times.append(end_time - start_time)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Average inference time per review: {sum(inference_times) / len(inference_times):.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
