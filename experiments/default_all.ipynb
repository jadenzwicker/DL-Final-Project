{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and setup Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pace Setup\n",
    "# !module load anaconda3\n",
    "# !module load gcc/12.3.0\n",
    "# !module load cuda/12.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  8 01:19:42 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          On  |   00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             42W /  250W |       1MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "/home/hice1/jzwicker3/DL-Final-Project/.env\n",
      "Hugging Face cache directory set to: /storage/ice1/3/5/jzwicker3/.cache/huggingface\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/3/5/jzwicker3/dl-fp/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.5.1 with cuda 12.4\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Update HF cache directory\n",
    "env_path = os.path.abspath(os.path.join(os.getcwd(), '..', '.env'))\n",
    "print(env_path)\n",
    "load_dotenv(env_path)\n",
    "hf_cache_dir = os.getenv('TRANSFORMERS_CACHE')\n",
    "os.makedirs(hf_cache_dir, exist_ok=True)\n",
    "print(f\"Hugging Face cache directory set to: {hf_cache_dir}\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check cuda version torch is using\n",
    "print(f\"Using torch {torch.__version__} with cuda {torch.version.cuda}\")\n",
    "\n",
    "workspace_dir = os.getenv('WORKSPACE_DIR')\n",
    "\n",
    "seed = 42\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset\n",
    "It is stored in the dataset directory which is gitignored so run this block to repopulate if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 25000\n",
      "Test dataset: 25000\n",
      "{'text': \"<br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br />Keep up the good work guys, you CAN and DO make a difference.<br /><br />\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# # Check if dataset is present\n",
    "# dataset_dir = os.path.join(workspace_dir, 'datasets')\n",
    "# os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# if not os.path.exists(os.path.join(dataset_dir, 'IMDB Dataset.csv')):\n",
    "#     !kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews --path {dataset_dir} --unzip\n",
    "\n",
    "# # Load dataset into dataframe\n",
    "# dataset = pd.read_csv(os.path.join(dataset_dir, 'IMDB Dataset.csv'))\n",
    "# print(dataset.head())\n",
    "\n",
    "# _, test_set = train_test_split(dataset, test_size=0.2, random_state=seed)\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "test_dataset = imdb['test'].shuffle(seed=seed)#.select([i for i in list(range(500))])\n",
    "train_dataset = imdb['train'].shuffle(seed=seed)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Experiment Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUGE NOTE, the order of pos vs neg in the asking of sentiment matters alot, first one listed tends to be the default models answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "/scratch/1030276/ipykernel_2832414/2902478909.py:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To calculate the values of accuracy, recall, specificity, precision, and F-score, you need the confusion matrix or the key components: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). Here's how each metric is calculated:\n",
    "\n",
    "1. **Accuracy**: The proportion of correctly classified instances (both positive and negative) out of all instances.\n",
    "   \\[\n",
    "   \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "   \\]\n",
    "\n",
    "2. **Recall (Sensitivity)**: The proportion of actual positives correctly identified.\n",
    "   \\[\n",
    "   \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "   \\]\n",
    "\n",
    "3. **Specificity**: The proportion of actual negatives correctly identified.\n",
    "   \\[\n",
    "   \\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "   \\]\n",
    "\n",
    "4. **Precision**: The proportion of predicted positives that are actually positive.\n",
    "   \\[\n",
    "   \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "   \\]\n",
    "\n",
    "5. **F-score**: The harmonic mean of precision and recall, balancing the two.\n",
    "   \\[\n",
    "   \\text{F-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   \\]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_model(model, tokenizer, device, dataset, top_k=50, shot_type='zero'):\n",
    "    # Data preparation\n",
    "    reviews = [example['text'] for example in dataset]\n",
    "    true_labels = [example['label'] for example in dataset]  # 0 for negative, 1 for positive\n",
    "\n",
    "    few_shot_rev_1 = \"Movie Review: I loved this movie ! So good plot ! \\n Only Answer if this Movie Review is Positive or Negative: Positive \\n\"\n",
    "    few_shot_rev_2 = \"Movie Review: I hated this, could be a lot better \\n Only Answer if this Movie Review is Positive or Negative: Negative \\n\"\n",
    "    few_shot_rev_3 = \"Movie Review: This move was so good I would recommend to all my friends! \\n Only Answer if this Movie Review is Positive or Negative: Positive \\n\"\n",
    "\n",
    "    # One function for both\n",
    "    if shot_type == 'zero':\n",
    "        # Define the prompts\n",
    "        prompts = [f\"Movie Review: {review} \\n Only Answer if this Movie Review is Positive or Negative:\" for review in reviews]\n",
    "    elif shot_type == 'few':\n",
    "        # Need to have prompts that do not ask if pos or neg vs neg vs pos since the model will just answer the first one\n",
    "        prompts = [f\"{few_shot_rev_1} {few_shot_rev_2} {few_shot_rev_3} Movie Review: {review} \\n Only Answer if this Movie Review is Positive or Negative:\" for review in reviews]\n",
    "\n",
    "    # Perform inference\n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "    idk_predictions = 0\n",
    "\n",
    "    for idx, example in tqdm(enumerate(dataset), total=len(dataset), desc=\"Processing\", leave=True):\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer.encode(prompts[idx], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Get the probabilities for the next token\n",
    "        next_token_logits = logits[:, -1, :]  # Only consider the last token's logits\n",
    "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Get the top k most likely tokens\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k) # these are sorted in order of most likely to least likely\n",
    "\n",
    "        # Decode the top k tokens\n",
    "        top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices[0]]\n",
    "\n",
    "        # print(f\"Top-k tokens for review {reviews[idx]}: {top_k_tokens}\")\n",
    "        \n",
    "        # Extract the sentiment prediction from the top k tokens, if the model did not predict a sentiment, default to negative\n",
    "        pred = -1\n",
    "        for token in top_k_tokens:\n",
    "            token_lower = token.strip().lower()\n",
    "            if token_lower == 'positive':\n",
    "                pred = 1\n",
    "                break\n",
    "            elif token_lower == 'negative':\n",
    "                pred = 0\n",
    "                break\n",
    "            \n",
    "        if pred == -1:\n",
    "            idk_predictions += 1\n",
    "            pred = 0\n",
    "\n",
    "        # If the model did not predict a sentiment, default to negative\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        \n",
    "    # print(predictions)\n",
    "    # Calculate confusion matrix    \n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Calculate true and false answer percentages\n",
    "    total_samples = len(true_labels)\n",
    "    true_percent = predictions.count(1) / total_samples * 100\n",
    "    false_percent = predictions.count(0) / total_samples * 100\n",
    "    \n",
    "    # Calculate total and average inference times\n",
    "    total_inference_time = sum(inference_times)\n",
    "    average_inference_time = total_inference_time / len(inference_times)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"precision\": precision,\n",
    "        \"f_score\": f_score,\n",
    "        \"true_percent\": true_percent,\n",
    "        \"false_percent\": false_percent,\n",
    "        \"unknown_predictions\": idk_predictions,\n",
    "        \"total_inference_time\": total_inference_time,\n",
    "        \"average_inference_time\": average_inference_time\n",
    "    }\n",
    "\n",
    "\n",
    "def create_results_table(results_dict, model_name=\"Model Results\"):\n",
    "    \"\"\"\n",
    "    Creates a formatted table from the results dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "        results_dict (dict): Dictionary containing evaluation metrics.\n",
    "        model_name (str): Name of the model being evaluated.\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted table as a string.\n",
    "    \"\"\"\n",
    "    # Initialize a PrettyTable\n",
    "    table = PrettyTable()\n",
    "    \n",
    "    # Set the table title\n",
    "    table.title = f\"Results for {model_name}\"\n",
    "    \n",
    "    # Add columns\n",
    "    table.field_names = [\"Metric\", \"Value\"]\n",
    "    \n",
    "    # Add rows for each metric\n",
    "    table.add_row([\"Accuracy\", f\"{results_dict['accuracy']:.2f}\"])\n",
    "    table.add_row([\"Recall (Sensitivity)\", f\"{results_dict['recall']:.2f}\"])\n",
    "#     if \"specificity\" in results_dict:  # Specificity might not be included in some results\n",
    "    table.add_row([\"Specificity\", f\"{results_dict['specificity']:.2f}\"])\n",
    "    table.add_row([\"Precision\", f\"{results_dict['precision']:.2f}\"])\n",
    "    table.add_row([\"F-Score\", f\"{results_dict['f_score']:.2f}\"])\n",
    "    table.add_row([\"% True Predictions\", f\"{results_dict['true_percent']:.2f}%\"])\n",
    "    table.add_row([\"% False Predictions\", f\"{results_dict['false_percent']:.2f}%\"])\n",
    "    table.add_row([\"Unknown Predictions\", f\"{results_dict['unknown_predictions']:.2f}%\"])\n",
    "    table.add_row([\"Total Inference Time (s)\", f\"{results_dict['total_inference_time']:.2f}\"])\n",
    "    table.add_row([\"Average Inference Time (s)\", f\"{results_dict['average_inference_time']:.2f}\"])\n",
    "    \n",
    "    # Return the table as a string\n",
    "    return table.get_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLM-135M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears cuda from last run\n",
    "# if device == \"cuda\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     torch.cuda.ipc_collect()\n",
    "#     torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "#FROM https://huggingface.co/HuggingFaceTB/SmolLM2-135M TODO: Dont forget to cite the model in report\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 25000/25000 [09:16<00:00, 44.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|   Results for SmolLM2-135M Zero-Shot  |\n",
      "+----------------------------+----------+\n",
      "|           Metric           |  Value   |\n",
      "+----------------------------+----------+\n",
      "|          Accuracy          |   0.50   |\n",
      "|    Recall (Sensitivity)    |   0.01   |\n",
      "|        Specificity         |   1.00   |\n",
      "|         Precision          |   0.78   |\n",
      "|          F-Score           |   0.02   |\n",
      "|     % True Predictions     |  0.78%   |\n",
      "|    % False Predictions     |  99.22%  |\n",
      "|    Unknown Predictions     | 2993.00% |\n",
      "|  Total Inference Time (s)  |  464.59  |\n",
      "| Average Inference Time (s) |   0.02   |\n",
      "+----------------------------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zero_135_results = evaluate_model(model, tokenizer, device, test_dataset, shot_type='zero')\n",
    "\n",
    "zero_135_table = create_results_table(zero_135_results, model_name=\"SmolLM2-135M Zero-Shot\")\n",
    "print(zero_135_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 25000/25000 [09:34<00:00, 43.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|  Results for SmolLM2-135M Few-Shot  |\n",
      "+----------------------------+--------+\n",
      "|           Metric           | Value  |\n",
      "+----------------------------+--------+\n",
      "|          Accuracy          |  0.59  |\n",
      "|    Recall (Sensitivity)    |  0.80  |\n",
      "|        Specificity         |  0.37  |\n",
      "|         Precision          |  0.56  |\n",
      "|          F-Score           |  0.66  |\n",
      "|     % True Predictions     | 71.86% |\n",
      "|    % False Predictions     | 28.14% |\n",
      "|    Unknown Predictions     | 0.00%  |\n",
      "|  Total Inference Time (s)  | 467.02 |\n",
      "| Average Inference Time (s) |  0.02  |\n",
      "+----------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "few_135_results = evaluate_model(model, tokenizer, device, test_dataset, shot_type='few')\n",
    "\n",
    "few_135_table = create_results_table(few_135_results, model_name=\"SmolLM2-135M Few-Shot\")\n",
    "print(few_135_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLM-360M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears cuda from last run\n",
    "# if device == \"cuda\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     torch.cuda.ipc_collect()\n",
    "#     torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 25000/25000 [12:23<00:00, 33.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|  Results for SmolLM2-360M Zero-Shot |\n",
      "+----------------------------+--------+\n",
      "|           Metric           | Value  |\n",
      "+----------------------------+--------+\n",
      "|          Accuracy          |  0.56  |\n",
      "|    Recall (Sensitivity)    |  1.00  |\n",
      "|        Specificity         |  0.11  |\n",
      "|         Precision          |  0.53  |\n",
      "|          F-Score           |  0.69  |\n",
      "|     % True Predictions     | 94.26% |\n",
      "|    % False Predictions     | 5.74%  |\n",
      "|    Unknown Predictions     | 2.00%  |\n",
      "|  Total Inference Time (s)  | 499.90 |\n",
      "| Average Inference Time (s) |  0.02  |\n",
      "+----------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "zero_360_results = evaluate_model(model, tokenizer, device, test_dataset, shot_type='zero')\n",
    "\n",
    "zero_360_table = create_results_table(zero_360_results, model_name=\"SmolLM2-360M Zero-Shot\")\n",
    "print(zero_360_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 25000/25000 [13:51<00:00, 30.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|  Results for SmolLM2-360M Few-Shot  |\n",
      "+----------------------------+--------+\n",
      "|           Metric           | Value  |\n",
      "+----------------------------+--------+\n",
      "|          Accuracy          |  0.66  |\n",
      "|    Recall (Sensitivity)    |  0.99  |\n",
      "|        Specificity         |  0.33  |\n",
      "|         Precision          |  0.60  |\n",
      "|          F-Score           |  0.75  |\n",
      "|     % True Predictions     | 82.74% |\n",
      "|    % False Predictions     | 17.26% |\n",
      "|    Unknown Predictions     | 0.00%  |\n",
      "|  Total Inference Time (s)  | 490.53 |\n",
      "| Average Inference Time (s) |  0.02  |\n",
      "+----------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "few_360_results = evaluate_model(model, tokenizer, device, test_dataset, shot_type='few')\n",
    "\n",
    "few_360_table = create_results_table(few_360_results, model_name=\"SmolLM2-360M Few-Shot\")\n",
    "print(few_360_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLM2-1.7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears cuda from last run\n",
    "# if device == \"cuda\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     torch.cuda.ipc_collect()\n",
    "#     torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_17_results = evaluate_model(model, tokenizer, device, test_dataset, shot_type='zero')\n",
    "\n",
    "zero_17_table = create_results_table(zero_17_results, model_name=\"SmolLM2-1.7B Zero-Shot\")\n",
    "print(zero_17_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_17_results = evaluate_model(model, tokenizer, device, test_dataset, shot_type='few')\n",
    "\n",
    "few_17_table = create_results_table(few_17_results, model_name=\"SmolLM2-1.7B Few-Shot\")\n",
    "print(few_17_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All models evaluated successfully!\")\n",
    "\n",
    "results_dicts = [\n",
    "    zero_135_results,\n",
    "    few_135_results,\n",
    "    zero_360_results,\n",
    "    few_360_results,\n",
    "    zero_17_results,\n",
    "    few_17_results,\n",
    "]\n",
    "\n",
    "# List of model names\n",
    "model_names = [\n",
    "    \"SmolLM2-135M Zero-Shot\",\n",
    "    \"SmolLM2-135M Few-Shot\",\n",
    "    \"SmolLM2-360M Zero-Shot\",\n",
    "    \"SmolLM2-360M Few-Shot\",\n",
    "    \"SmolLM2-1.7B Zero-Shot\",\n",
    "    \"SmolLM2-1.7B Few-Shot\",\n",
    "]\n",
    "\n",
    "table = PrettyTable()\n",
    "\n",
    "# Define the columns\n",
    "table.field_names = [\n",
    "    \"Model\",\n",
    "    \"Accuracy\",\n",
    "    \"Recall (Sensitivity)\",\n",
    "    \"Specificity\",\n",
    "    \"Precision\",\n",
    "    \"F-Score\",\n",
    "    \"% True Predictions\",\n",
    "    \"% False Predictions\",\n",
    "    \"Unknown Predictions\",\n",
    "    \"Total Inference Time (s)\",\n",
    "    \"Avg Inference Time (s)\"\n",
    "]\n",
    "\n",
    "# Populate the table\n",
    "for model_name, results in zip(model_names, results_dicts):\n",
    "    table.add_row([\n",
    "        model_name,\n",
    "        f\"{results['accuracy']:.2f}\",\n",
    "        f\"{results['recall']:.2f}\",\n",
    "        f\"{results['specificity']:.2f}\",\n",
    "        f\"{results['precision']:.2f}\",\n",
    "        f\"{results['f_score']:.2f}\",\n",
    "        f\"{results['true_percent']:.2f}%\",\n",
    "        f\"{results['false_percent']:.2f}%\",\n",
    "        f\"{results['unknown_predictions']:.2f}%\",\n",
    "        f\"{results['total_inference_time']:.2f}\",\n",
    "        f\"{results['average_inference_time']:.2f}\",\n",
    "    ])\n",
    "\n",
    "print(table)\n",
    "print(results_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models tend to say true more because they see it first in the prompt, but to counter this they do default to negative. MENTION IN REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears cuda from last run\n",
    "# if device == \"cuda\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     torch.cuda.ipc_collect()\n",
    "#     torch.cuda.reset_peak_memory_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl-fp]",
   "language": "python",
   "name": "conda-env-dl-fp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
