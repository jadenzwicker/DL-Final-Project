{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and setup Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face cache directory set to: /root/repos/DL-Final-Project/.cache/huggingface\n",
      "Using torch 2.5.1 with cuda 12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dl-fp-env/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BatchEncoding\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Update HF cache directory\n",
    "env_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..', '.env'))\n",
    "load_dotenv(env_path)\n",
    "hf_cache_dir = os.getenv('TRANSFORMERS_CACHE')\n",
    "os.makedirs(hf_cache_dir, exist_ok=True)\n",
    "print(f\"Hugging Face cache directory set to: {hf_cache_dir}\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check cuda version torch is using\n",
    "print(f\"Using torch {torch.__version__} with cuda {torch.version.cuda}\")\n",
    "\n",
    "workspace_dir = os.getenv('WORKSPACE_DIR')\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset\n",
    "It is stored in the dataset directory which is gitignored so run this block to repopulate if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 25000\n",
      "Test dataset: 100\n",
      "{'text': \"<br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br />Keep up the good work guys, you CAN and DO make a difference.<br /><br />\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# # Check if dataset is present\n",
    "# dataset_dir = os.path.join(workspace_dir, 'datasets')\n",
    "# os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# if not os.path.exists(os.path.join(dataset_dir, 'IMDB Dataset.csv')):\n",
    "#     !kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews --path {dataset_dir} --unzip\n",
    "\n",
    "# # Load dataset into dataframe\n",
    "# dataset = pd.read_csv(os.path.join(dataset_dir, 'IMDB Dataset.csv'))\n",
    "# print(dataset.head())\n",
    "\n",
    "# _, test_set = train_test_split(dataset, test_size=0.2, random_state=seed)\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "test_dataset = imdb['test'].shuffle(seed=seed).select([i for i in list(range(100))])\n",
    "train_dataset = imdb['train'].shuffle(seed=seed)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "/tmp/ipykernel_15902/800429644.py:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To calculate the values of accuracy, recall, specificity, precision, and F-score, you need the confusion matrix or the key components: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). Here's how each metric is calculated:\n",
    "\n",
    "1. **Accuracy**: The proportion of correctly classified instances (both positive and negative) out of all instances.\n",
    "   \\[\n",
    "   \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "   \\]\n",
    "\n",
    "2. **Recall (Sensitivity)**: The proportion of actual positives correctly identified.\n",
    "   \\[\n",
    "   \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "   \\]\n",
    "\n",
    "3. **Specificity**: The proportion of actual negatives correctly identified.\n",
    "   \\[\n",
    "   \\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "   \\]\n",
    "\n",
    "4. **Precision**: The proportion of predicted positives that are actually positive.\n",
    "   \\[\n",
    "   \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "   \\]\n",
    "\n",
    "5. **F-score**: The harmonic mean of precision and recall, balancing the two.\n",
    "   \\[\n",
    "   \\text{F-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   \\]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def evaluate_model_zero_shot(model, tokenizer, device, dataset, top_k=50):\n",
    "    # Data preparation\n",
    "    reviews = [example['text'] for example in dataset]\n",
    "    true_labels = [example['label'] for example in dataset]  # 0 for negative, 1 for positive\n",
    "\n",
    "    # Define the prompts\n",
    "    prompts = [f\"Movie Review: {review} \\n Only Answer if this Movie Review is Positive or Negative:\" for review in reviews]\n",
    "\n",
    "    # Perform inference\n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "\n",
    "    for idx, example in tqdm(enumerate(dataset), total=len(dataset), desc=\"Processing\", leave=True):\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer.encode(prompts[idx], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Get the probabilities for the next token\n",
    "        next_token_logits = logits[:, -1, :]  # Only consider the last token's logits\n",
    "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Get the top k most likely tokens\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k) # these are sorted in order of most likely to least likely\n",
    "\n",
    "        # Decode the top k tokens\n",
    "        top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices[0]]\n",
    "\n",
    "        # Extract the sentiment prediction from the top k tokens, default to negative for confusion matrix calculation\n",
    "        for token in top_k_tokens:\n",
    "            token_lower = token.strip().lower()\n",
    "            if token_lower == 'positive':\n",
    "                pred = 1\n",
    "                break\n",
    "            else:\n",
    "                pred = 0\n",
    "\n",
    "        # If the model did not predict a sentiment, default to negative\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "\n",
    "    # Calculate confusion matrix    \n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Calculate total and average inference times\n",
    "    total_inference_time = sum(inference_times)\n",
    "    average_inference_time = total_inference_time / len(inference_times)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"precision\": precision,\n",
    "        \"f_score\": f_score,\n",
    "        \"total_inference_time\": total_inference_time,\n",
    "        \"average_inference_time\": average_inference_time\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_few_shot(model, tokenizer, device, dataset, top_k=50):\n",
    "    # Data preparation\n",
    "    reviews = [example['text'] for example in dataset]\n",
    "    true_labels = [example['label'] for example in dataset]  # 0 for negative, 1 for positive\n",
    "\n",
    "    # Define the prompts\n",
    "    # example_review = \"This movie was an absolute masterpiece with stunning visuals and a gripping story!\"\n",
    "    # example_review_neg = \"This movie was terrible and I hated it.\"\n",
    "    # example_negative_review_2 = \"I really think this movie is not that good. It was a waste of time.\"\n",
    "    few_shot_rev_1 = \"Movie Review: I loved this movie ! So good plot ! \\n Only Answer if this Movie Review is Positive or Negative: Positive \\n\"\n",
    "    few_shot_rev_2 = \"Movie Review: I hated this, could be a lot better \\n Only Answer if this Movie Review is Positive or Negative: Negative \\n\"\n",
    " \n",
    "    prompts = [f\"{few_shot_rev_1} {few_shot_rev_2} Movie Review: {review} \\n Only Answer if this Movie Review is Positive or Negative:\" for review in reviews]\n",
    "\n",
    "    # Perform inference\n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "\n",
    "    for idx, example in tqdm(enumerate(dataset), total=len(dataset), desc=\"Processing\", leave=True):\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer.encode(prompts[idx], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Get the probabilities for the next token\n",
    "        next_token_logits = logits[:, -1, :]  # Only consider the last token's logits\n",
    "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Get the top k most likely tokens\n",
    "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k) # these are sorted in order of most likely to least likely\n",
    "\n",
    "        # Decode the top k tokens\n",
    "        top_k_tokens = [tokenizer.decode([token]) for token in top_k_indices[0]]\n",
    "\n",
    "        # Extract the sentiment prediction from the top k tokens\n",
    "        for token in top_k_tokens:\n",
    "            token_lower = token.strip().lower()\n",
    "            if token_lower == 'positive':\n",
    "                pred = 1\n",
    "                break\n",
    "            else:\n",
    "                pred = 0\n",
    "\n",
    "        # If the model did not predict a sentiment, default to negative\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "    # Calculate confusion matrix    \n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Calculate total and average inference times\n",
    "    total_inference_time = sum(inference_times)\n",
    "    average_inference_time = total_inference_time / len(inference_times)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"precision\": precision,\n",
    "        \"f_score\": f_score,\n",
    "        \"total_inference_time\": total_inference_time,\n",
    "        \"average_inference_time\": average_inference_time\n",
    "    }\n",
    "\n",
    "\n",
    "def create_results_table(results_dict, model_name=\"Model Results\"):\n",
    "    \"\"\"\n",
    "    Creates a formatted table from the results dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "        results_dict (dict): Dictionary containing evaluation metrics.\n",
    "        model_name (str): Name of the model being evaluated.\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted table as a string.\n",
    "    \"\"\"\n",
    "    # Initialize a PrettyTable\n",
    "    table = PrettyTable()\n",
    "    \n",
    "    # Set the table title\n",
    "    table.title = f\"Results for {model_name}\"\n",
    "    \n",
    "    # Add columns\n",
    "    table.field_names = [\"Metric\", \"Value\"]\n",
    "    \n",
    "    # Add rows for each metric\n",
    "    table.add_row([\"Accuracy\", f\"{results_dict['accuracy']:.2f}\"])\n",
    "    table.add_row([\"Recall (Sensitivity)\", f\"{results_dict['recall']:.2f}\"])\n",
    "    if \"specificity\" in results_dict:  # Specificity might not be included in some results\n",
    "        table.add_row([\"Specificity\", f\"{results_dict['specificity']:.2f}\"])\n",
    "    table.add_row([\"Precision\", f\"{results_dict['precision']:.2f}\"])\n",
    "    table.add_row([\"F-Score\", f\"{results_dict['f_score']:.2f}\"])\n",
    "    table.add_row([\"Total Inference Time (s)\", f\"{results_dict['total_inference_time']:.2f}\"])\n",
    "    table.add_row([\"Average Inference Time (s)\", f\"{results_dict['average_inference_time']:.2f}\"])\n",
    "    \n",
    "    # Return the table as a string\n",
    "    return table.get_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLM-135M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "# Clears cuda from last run\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "#FROM https://huggingface.co/HuggingFaceTB/SmolLM2-135M TODO: Dont forget to cite the model in report\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f089b0b017b24af3890de94824163bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|  Results for SmolLM2-135M Zero-Shot  |\n",
      "+-----------------------------+--------+\n",
      "|            Metric           | Value  |\n",
      "+-----------------------------+--------+\n",
      "|           Accuracy          |  0.64  |\n",
      "|     Recall (Sensitivity)    |  0.51  |\n",
      "|         Specificity         |  0.75  |\n",
      "|          Precision          |  0.65  |\n",
      "|           F-Score           |  0.57  |\n",
      "|   Total Inference Time (s)  |  1.87  |\n",
      "|  Average Inference Time (s) |  0.02  |\n",
      "+-----------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "zero_135_results = evaluate_model_zero_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "zero_135_table = create_results_table(zero_135_results, model_name=\"SmolLM2-135M Zero-Shot\")\n",
    "print(zero_135_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eba62ca351848399c122a1b426741d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "| Results for SmolLM2-135M Few-Shot  |\n",
      "+----------------------------+-------+\n",
      "|           Metric           | Value |\n",
      "+----------------------------+-------+\n",
      "|          Accuracy          |  0.47 |\n",
      "|    Recall (Sensitivity)    |  1.00 |\n",
      "|        Specificity         |  0.00 |\n",
      "|         Precision          |  0.47 |\n",
      "|          F-Score           |  0.64 |\n",
      "|  Total Inference Time (s)  |  1.81 |\n",
      "| Average Inference Time (s) |  0.02 |\n",
      "+----------------------------+-------+\n"
     ]
    }
   ],
   "source": [
    "few_135_results = evaluate_model_few_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "few_135_table = create_results_table(few_135_results, model_name=\"SmolLM2-135M Few-Shot\")\n",
    "print(few_135_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLM-360M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears cuda from last run\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8ca15c1f384fb3997d9038a13aa4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|  Results for SmolLM2-360M Zero-Shot  |\n",
      "+-----------------------------+--------+\n",
      "|            Metric           | Value  |\n",
      "+-----------------------------+--------+\n",
      "|           Accuracy          |  0.47  |\n",
      "|     Recall (Sensitivity)    |  1.00  |\n",
      "|         Specificity         |  0.00  |\n",
      "|          Precision          |  0.47  |\n",
      "|           F-Score           |  0.64  |\n",
      "|   Total Inference Time (s)  |  1.90  |\n",
      "|  Average Inference Time (s) |  0.02  |\n",
      "+-----------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "zero_360_results = evaluate_model_zero_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "zero_360_table = create_results_table(zero_360_results, model_name=\"SmolLM2-360M Zero-Shot\")\n",
    "print(zero_360_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee70baabc994ceb8ab703dc451d9f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "| Results for SmolLM2-360M Few-Shot  |\n",
      "+----------------------------+-------+\n",
      "|           Metric           | Value |\n",
      "+----------------------------+-------+\n",
      "|          Accuracy          |  0.47 |\n",
      "|    Recall (Sensitivity)    |  1.00 |\n",
      "|        Specificity         |  0.00 |\n",
      "|         Precision          |  0.47 |\n",
      "|          F-Score           |  0.64 |\n",
      "|  Total Inference Time (s)  |  1.80 |\n",
      "| Average Inference Time (s) |  0.02 |\n",
      "+----------------------------+-------+\n"
     ]
    }
   ],
   "source": [
    "few_360_results = evaluate_model_few_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "few_360_table = create_results_table(few_360_results, model_name=\"SmolLM2-360M Few-Shot\")\n",
    "print(few_360_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmolLM2-1.7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clears cuda from last run\n",
    "# if device == \"cuda\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     torch.cuda.ipc_collect()\n",
    "#     torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "# device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9907dbd2c154c249ca76adffadf18a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|  Results for SmolLM2-1.7B Zero-Shot  |\n",
      "+-----------------------------+--------+\n",
      "|            Metric           | Value  |\n",
      "+-----------------------------+--------+\n",
      "|           Accuracy          |  0.47  |\n",
      "|     Recall (Sensitivity)    |  1.00  |\n",
      "|         Specificity         |  0.00  |\n",
      "|          Precision          |  0.47  |\n",
      "|           F-Score           |  0.64  |\n",
      "|   Total Inference Time (s)  |  1.89  |\n",
      "|  Average Inference Time (s) |  0.02  |\n",
      "+-----------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "zero_17_results = evaluate_model_zero_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "zero_17_table = create_results_table(zero_17_results, model_name=\"SmolLM2-1.7B Zero-Shot\")\n",
    "print(zero_17_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76aa86521d654db9947a7a5a058264a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "| Results for SmolLM2-1.7B Few-Shot  |\n",
      "+----------------------------+-------+\n",
      "|           Metric           | Value |\n",
      "+----------------------------+-------+\n",
      "|          Accuracy          |  0.47 |\n",
      "|    Recall (Sensitivity)    |  1.00 |\n",
      "|        Specificity         |  0.00 |\n",
      "|         Precision          |  0.47 |\n",
      "|          F-Score           |  0.64 |\n",
      "|  Total Inference Time (s)  |  1.89 |\n",
      "| Average Inference Time (s) |  0.02 |\n",
      "+----------------------------+-------+\n"
     ]
    }
   ],
   "source": [
    "few_17_results = evaluate_model_few_shot(model, tokenizer, device, test_dataset)\n",
    "\n",
    "few_17_table = create_results_table(few_17_results, model_name=\"SmolLM2-1.7B Few-Shot\")\n",
    "print(few_17_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models evaluated successfully!\n",
      "+------------------------+----------+----------------------+-------------+-----------+---------+--------------------------+------------------------+\n",
      "|         Model          | Accuracy | Recall (Sensitivity) | Specificity | Precision | F-Score | Total Inference Time (s) | Avg Inference Time (s) |\n",
      "+------------------------+----------+----------------------+-------------+-----------+---------+--------------------------+------------------------+\n",
      "| SmolLM2-135M Zero-Shot |   0.64   |         0.51         |     0.75    |    0.65   |   0.57  |           1.87           |          0.02          |\n",
      "| SmolLM2-135M Few-Shot  |   0.47   |         1.00         |     0.00    |    0.47   |   0.64  |           1.81           |          0.02          |\n",
      "| SmolLM2-360M Zero-Shot |   0.47   |         1.00         |     0.00    |    0.47   |   0.64  |           1.90           |          0.02          |\n",
      "| SmolLM2-360M Few-Shot  |   0.47   |         1.00         |     0.00    |    0.47   |   0.64  |           1.80           |          0.02          |\n",
      "| SmolLM2-1.7B Zero-Shot |   0.47   |         1.00         |     0.00    |    0.47   |   0.64  |           1.89           |          0.02          |\n",
      "| SmolLM2-1.7B Few-Shot  |   0.47   |         1.00         |     0.00    |    0.47   |   0.64  |           1.89           |          0.02          |\n",
      "+------------------------+----------+----------------------+-------------+-----------+---------+--------------------------+------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(\"All models evaluated successfully!\")\n",
    "\n",
    "results_dicts = [\n",
    "    zero_135_results,\n",
    "    few_135_results,\n",
    "    zero_360_results,\n",
    "    few_360_results,\n",
    "    zero_17_results,\n",
    "    few_17_results,\n",
    "]\n",
    "\n",
    "# List of model names\n",
    "model_names = [\n",
    "    \"SmolLM2-135M Zero-Shot\",\n",
    "    \"SmolLM2-135M Few-Shot\",\n",
    "    \"SmolLM2-360M Zero-Shot\",\n",
    "    \"SmolLM2-360M Few-Shot\",\n",
    "    \"SmolLM2-1.7B Zero-Shot\",\n",
    "    \"SmolLM2-1.7B Few-Shot\",\n",
    "]\n",
    "\n",
    "table = PrettyTable()\n",
    "\n",
    "# Define the columns\n",
    "table.field_names = [\n",
    "    \"Model\",\n",
    "    \"Accuracy\",\n",
    "    \"Recall (Sensitivity)\",\n",
    "    \"Specificity\",\n",
    "    \"Precision\",\n",
    "    \"F-Score\",\n",
    "    \"Total Inference Time (s)\",\n",
    "    \"Avg Inference Time (s)\"\n",
    "]\n",
    "\n",
    "# Populate the table\n",
    "for model_name, results in zip(model_names, results_dicts):\n",
    "    table.add_row([\n",
    "        model_name,\n",
    "        f\"{results['accuracy']:.2f}\",\n",
    "        f\"{results['recall']:.2f}\",\n",
    "        f\"{results['specificity']:.2f}\",\n",
    "        f\"{results['precision']:.2f}\",\n",
    "        f\"{results['f_score']:.2f}\",\n",
    "        f\"{results['total_inference_time']:.2f}\",\n",
    "        f\"{results['average_inference_time']:.2f}\",\n",
    "    ])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears cuda from last run\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
